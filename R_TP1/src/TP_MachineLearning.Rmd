---
title: "TP I FSBM UE 12: Big Data and predictive models"
subtitle: "Introduction to machine learning."
author:
  - Yoann Pradat, PhDc yoann.pradat@centralesupelec.fr
  - Julien Vibert, Md, PhDc Julien.vibert@gustaveroussy.fr
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: references.bib
link-citations: true
output:
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    code_folding: show
  rmdformats::material:
    highlight: kate
---

<style>
body {
text-align: justify}
</style>

```{r, echo = FALSE, out.width = '50%', fig.align="center"}
knitr::include_graphics('../../img/ifsbm_logo.jpeg')
```

# 1. What you will do in this R Notebook

## 1.1 Introduction
With this notebook you will work on **real cancer genomic data** to address clinical questions.  The problem can be
summarized this way: your input is a huge amount of variables, such as DNA or RNA sequences for each patient, that you
wish to use in order to provide the clinician with a simple information that he can use for his patients.

The main challenge is how to **reduce information so that it becomes understandable a for a human**. That is a central objective in many machine learning projects.

Extracting biomedical knowledge from high-dimensional molecular data is currently part of the main challenges of
personalized medecine, so let's hope you'll enjoy this introductive notebook and that it can help you to address your current and/or future research challenges.

## 1.2 Main steps

In this notebook, you will be guided through TCGA data. We shall download only parts of the data in order to be able to
run jobs within a reasonable amount of time. To achieve this we will try to make the most of prior biomedical research
and reduce our queries to most relevant genes. 

In a first step we will implement simple modelling strategies to perform a classification task from expression data.
In a second step, if there is enough for it, we will try to run a survival analysis so as to give you a general idea of
the challenges of such analyses. 

It is where the current science stands! To go further you'll have to become a researcher in bioinformatics and machine learning. 

## 1.3 In practice

The code is provided to you. You will just have to follow the instructions all along the notebook.
Explanations and corrections will be given throughout the notebook for a subset of the questions.

**IMPORTANT**
There are questions in this notebook that you need to answer and code that you need to write by yourself during the lab
or at home. The ones you have to do during the lab are be indicated by a tag "INCLASS WORK", and the ones you have to
after the class are indicated by "HOME WORK". 

You will have to **complete both** and send the completed notebooks back to us. These completed notebooks will be
used for evaluating your work in this module and give you a mark.

# 2 Get familiar with R notebooks

## 2.1 What is an R notebook

An  *R notebook* is an R Markdown document (*.Rmd*) with text parts and R code parts (called *chunks*) that can be
executed independently and interactively. R notebooks can be rendered (or knit) into different formats (*.pdf*, *.html*,
*.docx*) using a renderer.  The `rmarkdown` R package ([see doc
here](https://cran.r-project.org/web/packages/rmarkdown/rmarkdown.pdf)) provides a lot of functions to render a
notebook. 

Running `render('TP_MachineLearning.Rmd', html_document())` from the R console will create a
file *TP_MachineLearning.html*  that you can then open with your favorite web browser. 

`RStudio` already provides you with commands to render your R notebook by taking into accounts the headers that are
specified at the top of document. You can knit your notebook with the `Preview` button or by pressing `Ctrl+Shift+K` (OS X: `Cmd+Shift+K`).

*Pratical info*: A list of all the keyboard shortcuts for `Rstudio` is available [here](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts).

Here is an example of YAML headers for specifying the output format.

```yaml
title: Nineteen Years Later
author: Harry Potter
date: July 31, 2016
output:
  rmarkdown::html_document:
    theme: lumen
```

## 2.2 Execute code your first **chunk**

Try executing the following chunk by clicking the `Run` button within the chunk or by placing your cursor inside it and
pressing `Ctrl+Enter` from within `R Studio`.

```{r}
print("Welcome to 'Big Data et predictive models' lab session")
```

In order to create a new chunk, open it with "\`\`\`{r}" and close it with "\`\`\`". Alternatively, press `Ctrl + Alt + I` (OS X:` Cmd + Option + I`) or the Add Chunk button in the tool bar of `R Studio`. Chunks rendering can by customized by specifying chunk options among the following

- `include = FALSE` prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
- `echo = FALSE` prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
- `message = FALSE` prevents messages that are generated by code from appearing in the finished file.
- `warning = FALSE` prevents warnings that are generated by code from appearing in the finished.
- `fig.cap = "..."` adds a caption to graphical results.
- `fig.width = 4` width of the graphical results
- `fig.height = 3` height of the graphical results
- `fig.align = "center"` height of the graphical results
- `dev = "png"` the graphical device to generate plots
- ...

Options can also be set from within the chunk using the syntax `knitr::opts_chunk$set()`. For instance, `knitr::opts_chunk$set(fig.width = 6, fig.height = 6)` will set the figure height and width. For more details about chunk options, see the online documentation [https://yihui.org/knitr/options/](https://yihui.org/knitr/options/).

**INCLASS WORK**: Create an `R` chunk that displays a plot of the cosinus and sinus functions on $[-\pi, \pi]$
withouth showing the code, with a caption, with a width and height of 8 and 4 respectively. The plot should additionally
be centered.

```{r, echo=FALSE, fig.cap="Cosinus and sinus plot", fig.height=4, fig.width=8, fig.align="center"}
x <- seq(-pi, pi, length.out=100)
ycos <- cos(x)
ysin <- sin(x)
plot(x, ycos)
lines(x, ysin)
```

# 3. Classification using expression data.

Over the course of the three previous decades, a lot of studies were conducted in which molecular data was collected on
individuals with particular medical conditions (a specific cancer type for instance) in order to establish a **molecular
profile** of these conditions.

The international consortium that overviews The Cancer Genome Atlas Project (TCGA) has for this purpose collected data
about over 11,000 patients with a cancer in one of 33 types.  The collected data includes **DNA sequencing data** (SNV,
SV, CNA), **RNA sequencing data** (microarrays and RNA-seq, providing gene expression tables), **Methylation data**,
**miRNA sequencing data**, **Proteomics data** and **Clinical data**. This huge project has opened a lot of new research
directions and continues to fuel active research.

The anonymized data is either in restricted or public access. To access the raw data (sequencing reads) you will need
the authorization of a data user commitee. To access processed data (gene expression tables), you don't need any
authorization. It can nevertheless be difficult to retrieve this data by hand and then load it into R. 

In this part of the notebook we will conduct a classification experiment using **expression data** from the TCGA. As you
may know, the human genome contains tens of thousands of genes giving rise to gene expression tables whose storage may
required close to 10Gb for the largest cohorts. For practical reasons, we shall only retrieve expression
data for specific lists of genes.

## 3.1 About the Cancer Gene Census

Using mutation data and by performing statistical tests to highlight signals of positive
selection or signals of non-random distributions of mutations in certain genes, researchers have established many list
of "**cancer genes**" or "**driver genes**" whose alterations are thought to be cancer-causing. Many statistical methods
were developed to identify these genes with often **a relatively poor overlap between methods** (see reviewing work in
[@tokheim_evaluating_2016]). However there a list of genes, called the Cancer Gene Census, that is manually curated to
include only those genes for which their role is tumorigenesis is clear. This list is used by a lot of methods as a
groundtruth to compare against and in many biomedical studies to uncover new profiles.

Since it began in 2004 [@futreal_census_2004], the **Cancer Gene Census** has established a list of over 700 genes  with
a comprehensive description of all the evidence for their involvement in cancer. The CGC teams lead a very thorough
curation process for including only genes for which the evidence is unequivocal. The latest paper from the group in
2018 [@tamborero_cancer_2018] presents the curation process in details and the categorization of genes into **two tier
lists**.

- Tier 1: genes that "possess a documented and reproducible activity relevant to cancer, along with evidence of
  mutations in cancer that change the activity of the gene product in a way that promotes oncogenic transformation"
- Tier 2: "genes with more recently identified roles in oncology, and consists of genes with strong indications of a
  role in cancer but with less strong mechanistic or functional evidence"

The Cancer Gene Census has already been downloaded for you and is available in the `R_TP1/data` folder under the name
`CancerGeneCensusCOSMIC_20210120.csv`.

## 3.2 Load expression data from the TCGA

For the purpose of this notebook, we have defined a function that will load the data you need from a list of genes and
cancer types. The processed data comes from [cBioportal](http://www.cbioportal.org/), a reference website widely used
for the exploration and downloading of data used in research papers.

For that you will need the official cBioportal R package [cgdsr](http://www.cbioportal.org/rmatlab) (already included for
you in the `renv` environment) and the function `LoadcBioportal.R` available in the `lib` folder of the github repository (if you are curious you can go and check the source code of this function).

To load the function, run the following

```{r}
source("../../lib/LoadcBioportal.R")
```

For the next steps we will be using `R` libraries. Load them with the following

```{r, message=FALSE}
library(RColorBrewer)
library(glmnet)
library(knitr)
```

Some useful functions.

```{r}
getColors <- function(vec, pal="Dark2", alpha=0.7){
  colors <- list()
  palette <- brewer.pal(max(length(unique(vec)),3), pal)
  i <- 1
  for (v in unique(vec)){
    colors[[v]] <- adjustcolor(palette[i], alpha)
    i <- i+1
  }
  colors
}
```


A R function may take as input one or multiple arguments that you may change depending on the task you want to perform.
The function allows you to query various tumor types existing from the TCGA. You can use either the official TCGA nomenclature (luad = lung adenocarcinoma, gbm = glioblastoma), or the name of the organ of origin (lung = luad & lusc, brain = gbm & lgg).

**INCLASS WORK** Use the `read.csv` function on the Cancer Gene Census file
`../data/CancerGeneCensusCOSMIC_20210120.csv` to load the names of cancer driver genes and use the `LoadcBioportal`
function to load the TCGA RNA and clinical data for `LUAD` and `GBM` TCGA cancer studies. The RNA data should be
restricted to cancer driver genes.

*Hint*: In regular expression (i.e computer langage), the "OR" logical connector is writen `|`. Moreoever, `T` stands
for `TRUE` and `F` stands for `FALSE`. Look at the arguments to understand the possibilities of the `LoadcBioportal`
function.

```{r}
# YOUR WORK HERE
```

Observe that you didn't load mutation data because of the argument `MutNeeded = F` so the dimensions of the `TcgaData$MUT` is [0,0].
The function has selected and sorted the patients so that you have a `TcgaData$EXP` and `TcgaData$CLINIC` with the same patients as rows and no NAs.

## 3.3 Description of the data

The first very important step is to visualize and describe your data. It can be tricky when you have huge multidimensional matrices. 

For continuous data such as RNA-seq, simply plotting the distribution is a good first step. Observe that the
`LoadcBioportal` function has performed some processing of the RNA data because of the argument `NormalizeRNA = T`.
The following chunk display the distribution of normalized gene expression across patients and genes for GBM and LUAD.

```{r}
hist(x=as.matrix(TcgaData$EXP[TcgaData$CLINIC$study=="luad",]), col=colorsStudy[["luad"]],
     main="Distribution of gene expressions", xlab="Normalized counts", freq = F)
hist(as.matrix(TcgaData$EXP[TcgaData$CLINIC$study=="gbm",]), add=T, col=colorsStudy[["gbm"]], freq = F)
legend("topright",legend=c("luad","gbm"), fill=2:3, cex=0.8)
```

A very popular visualisation tool for data matrices are heatmaps. However you may draw a heatmap only if you do not have
too many samples and too many variables. The `heatmap` base function additionally allows you to perform hierarchical
clustering of the rows and or the columns to highlight clusters of variables and or samples.

```{r, fig.cap="TCGA expression data LUAD and GBM", fig.height=8, fig.width=8, fig.align="center"}
rowNames <- rownames(TcgaData$EXP)
rowStudy <- TcgaData$CLINIC[rowNames, "study"]
rowSideColors <- sapply(rowStudy, function(x) colorsStudy[[x]])
heatmap(as.matrix(TcgaData$EXP), Rowv=NULL, Colv=NULL, keep.dendro=F, RowSideColors=rowSideColors)
```

**INCLASS_WORK** Answer to the numbered questions throughout the notebook.

&nbsp;

1) What are TPM and RPKM? If you were to use `LoadcBioportal` with `NormalizeRNA=F`, what would the RNA expression
numbers represent?

**ANSWER**:

&nbsp;

2) How would you qualify the distribution of the log-min-max normalized RNA data?

**ANSWER**:

&nbsp;

Another popular way of visualising high-dimensional data is first by reducing its dimension and then visualise it into
the lower-dimensional representation. The **Principal Component Analysis** with $k$ components allows you to find the
$k$ dimensional subspace that retains the most of the variance of the data in the original space. PCA of $\mathbf{X}$
may be performed by performing **Singular Value Decomposition** on the centered matrix.

```{r}
X <- t(t(TcgaData$EXP) - rowMeans(t(TcgaData$EXP)))
resSVD <- svd(X, nu=2, nv=2)
# the scores are the principal components i.e the 
# low-dimensional representations of the samples
scores <- resSVD$u %*% diag(resSVD$d[1:2])
```

```{r,fig.cap="PCA TCGA expression data LUAD and GBM", fig.height=6, fig.width=6, fig.align="center"}
# plot the two first principal components
plot(scores[,1],scores[,2],
     col=unlist(colorsStudy[TcgaData$CLINIC$study]), 
     pch=16,main='PCA representation',
     xlab=paste("dim1 = ",100*round(resSVD$d[1]^2/sum(resSVD$d^2),3),"%",sep = ""),
     ylab=paste("dim2 = ",100*round(resSVD$d[2]^2/sum(resSVD$d^2),3),"%",sep = ""))
legend("bottomright",legend=c("luad","gbm"), pch=16, cex=0.8, col=2:3)
```

## 3.4 Your first classification model

### Train/test split

We would like to have a model to identify the study of origin of the RNA samples from their gene expression. We shall
continue to rely on the gene expression of only the Cancer Gene Census. In here we are going to fit a `logistic
regression` model that we saw this morning to perform this classification task.

To begin with, we shall split the data into a training set and a test set.

```{r}
studySize <- nrow(TcgaData$CLINIC)
trainProp <- 0.8
trainIndex <- sample(seq(studySize), size=round(studySize*trainProp))
testIndex <-seq(studySize)[!seq(studySize) %in% trainIndex]

print(paste("Training size:", length(trainIndex)))
print(paste("Test size:", length(testIndex)))
```

3) Why is it important to split the data into a training set and a test set?

**ANSWER**:

&nbsp;

### Fit the model.

The [glmnet](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf) R package is a great tool for fitting all
kinds of linear models. GLM stands for for **Generalized Linear Models** which a global family of models that encompass
many linear models including the linear regression (for continuous predictions), logistic regression (for class
predictions), Poisson regression (for count data), Gamma regression (log normal data), etc. **Logistic
regression** is GLM from the **binomial family** of distributions with a **logit link**.

**INCLASS WORK** Use the [glmnet](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf) R package to fit a logistic
regression model on the expression data to predict the study membership of RNA samples.


```{r}
# YOUR WORK HERE
```

4) How do you evaluate a classification model? Give at least two metrics you think of to describe the quality of the
model.

**ANSWER**

&nbsp;

**INCLASS WORK** Evaluate your model on the *both* the train and test data by computing [confusion
matrix](https://en.wikipedia.org/wiki/Confusion_matrix) and the metrics you mentioned in your answer.

```{r}
# YOUR WORK HERE
```

### Interpret the model.

The medical doctor you are working is happy to see that you can accurately classify RNA samples according to their
organ/study of origin. He would like however to understand how the model works and in particular see which genes are the
most discriminative.

**INCLASS WORK** Extra the coefficients of the model using `coefficients(fit)` and propose a way to visualize the 20 top
most discriminative genes for classifying between LUAD and GBM (20 genes discriminative for LUAD, 20 genes
discriminative for GBM).

```{r}
#### YOUR WORK HERE
```

5) For the top most discriminative gene in favor of LUAD and the top most discriminative gene in favor of GBM, give the
average value and standard deviation of the number of reads associated observed in LUAD and GBM RNA samples
respectively. You will need to reuse `LoadcBioportal` with `NormalizeRNA=FALSE` to answer this question.

**ANSWER**:

6) Can you say these best 40 gene expressions can determine the tumor type as well as the 714 genes with no NAs from the
Cancer Gene Census?

**ANSWER**:

7) Are you allowed to refit a model using these top 40 genes and report the results of your reduced model in a research
paper ignoring how you came to select these genes? Why?

**ANSWER**:

## 3.5 A better classification model

Say you want to have a limited list of important genes in your model. It can be useful if a biotech wants to develop a
gene signature to determine the tissue of origin of a cancer sample, for example. It is more or less a regularization
(or penalization) problem: you want penalize models that have a too high number of discriminative genes (= high absolute
value of coefficients for **normalized** data).

An existing procedure to penalize models with too high coefficients values is the **lasso regularization**. Lasso (like
other regularization procedures) comes with an additional parameter to optimize, called the regularization parameter
$\lambda$. The recommended way to find $\lambda$ is with a **k-fold cross validation strategy**.

**INCLASS WORK**: Use the `cv.glmnet` function from `glmnet` to fit a lasso logistic regression model. The
cross-validation is here to help us find an optimal value of $\lambda$.

```{r}
#### YOUR WORK HERE
```

8) Do you think this model needs important regularization to generalize well, and why (max 2 reasons)?

**ANSWER**

&nbsp;

**YOUR WORK** Rerun the lasso logistic regression using the optimal values of $\lambda$, evaluate its quality. Show 
again the values of the coefficients of the top 20 most discriminative genes for both `GBM` and `LUAD`.

```{r}
#### YOUR WORK HERE
```


10) Has Lasso regularization changed the accuracy of your model? Are you surprised?

**ANSWER**:


11) If you go to see a clinician and tell him about your new model, do you think he will want to use it?

**ANSWER**: 

## 3.6 Home work

You will try a clinically more difficult task, namely that of discriminating between pancreatic adenocarcinomas and
cholangiocarninomas. This is difficult because: 

- these are rare tumors, few examples are available publicly.
- biliary tract and pancreas tumors can be very close to each other from anatomical and pathological point of views.


Find [here](https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/tcga-study-abbreviations) about TCGA 
nomenclatures of tumor types.

**HOME WORK** In the previous analysis we restricted ourselves to genes from the Cancer Gene Census and we used only
a linear model. In order to try to improve our model, you can choose one of the following extensions of the work we
performed.

1. Fit a non linear model using the gene expression data of the pancreatic adenocarcinomas and cholangiocarninoma. You can use any non-linear model that can perform classification (RandomForest, SVM with non-linear kernel, Neural Networks, Xgboost, etc.). You shall follow the same 
steps as we did here.
  + Train/test split
  + Fit on the train, evaluate on train and test. Assess the possible overfitting.
  + Try to determine which genes are the most discriminative. This task is optional if you use complex models that do
     not have an easy interpretation. However, RandomForest and Adaboost/Xgboost models allow to estimate the
     "importance" of features which you can use to rank features.
  
2. Fit a liner model (or any other you want) on the expression data of the pancreatic adenocarcinomas and
   cholangiocarninoma summarized into pathways. You can use gene lists from the [MSigDB
   database](http://www.gsea-msigdb.org/gsea/msigdb/index.jsp) (for the instance the 50 cancer pathways
   hallmarks). You shall retrieve expression data for all the genes that are in at least one of the pathways and
   aggregate the expression by taking the average expression of all genes in a pathway. Then follow the same analysis
   steps as we did. You can use [msigdbr](https://cran.r-project.org/web/packages/msigdbr/vignettes/msigdbr-intro.html)
   R package for this task.

```{r}
#### YOUR WORK HERE
```

# 4. Brief introduction to survival analysis

## 4.1 Load the data

```{r}
library(survival)
```


The function `LoadcBioportal` also loads the survival data from TCGA patients when available. 

```{r}
TcgaData <- LoadcBioportal(Genes = CgcGenes, Organ = "luad", 
                           ClinicNeeded = T, MutNeeded = F, 
                           RNANeeded = T, NormalizeRNA = T, 
                           FunctionalAnnot = F, Tests=T)

TcgaData$CLINIC[TcgaData$CLINIC$OS_STATUS=="0:LIVING", "OS_STATUS"] <- 0
TcgaData$CLINIC[TcgaData$CLINIC$OS_STATUS=="1:DECEASED", "OS_STATUS"] <- 1
TcgaData$CLINIC["OS_STATUS"] <- as.numeric(TcgaData$CLINIC$OS_STATUS)
```

Unfortunatly, there are some NAs and form some patients in TCGA data that will handle by just discarding them. This is acceptable because there is a low number of such cases. It would not be suitable otherwise.

```{r}
patientsWithNAs <- rowSums(is.na(TcgaData$CLINIC)) > 0
TcgaData$CLINIC <- TcgaData$CLINIC[!patientsWithNAs,]
TcgaData$EXP <- TcgaData$EXP[!patientsWithNAs,]

patientsWithZeroOS <- TcgaData$CLINIC$OS_MONTHS == 0
TcgaData$CLINIC <- TcgaData$CLINIC[!patientsWithZeroOS,]
TcgaData$EXP <- TcgaData$EXP[!patientsWithZeroOS,]
```

```{r}
print(Surv(TcgaData$CLINIC$OS_MONTHS, TcgaData$CLINIC$OS_STATUS))
```

## 4.2 Non parametric probability of survival
 
The `survfit` function of the survival package compute the cumulative probability of survival that takes into acount censored data in the calculation. At each time step $l$, the cumulative probability $P_{l}$ is calculated by:

$P_{l} = P_{l-1} \cdot \left( \frac{ NatRisk_{l}- Ndeath_{l} }{ NatRisk_{l}} \right)$

```{r}
survival <- survfit(Surv(TcgaData$CLINIC$OS_MONTHS, TcgaData$CLINIC$OS_STATUS)~NULL)
summary(survival)
```

The cumulative probability of survival allow you to plot the Kaplan-Meier curve, which is a classical way to visualize the distribution of the survival data accross the cohort.

```{r}
# plot survival data
plot(survfit(Surv(time = TcgaData$CLINIC$OS_MONTHS, event = TcgaData$CLINIC$OS_STATUS)~NULL),mark.time = T, 
     main="Kaplan Meier curves per organs", 
     ylab="Probability of survival", xlab="Time in months")
```

## 4.3 A semi-parametric model survival: the Cox model

Previously your task was to classify the histologies from expression data. Your new task is to predict survival of patients, still from the gene expression.

Modeling survival data for various parameters is a historical discipline (the Cox model has been published in 1952). With the emergence of molecular analysis of cancers it has become a big challenge worldwide. As you will see in this exemple it is much more complicated than histology classification.

Modeling survival looks like a regression task because you have numeric data to predict. However the way to perform your regression is adapted to the presence of censored data. The Cox model is using each variable (here gene expression) to model survival data .

```{r}
studySize <- nrow(TcgaData$CLINIC)
trainProp <- 0.8
trainIndex <- sample(seq(studySize), size=round(studySize*trainProp))
testIndex <-seq(studySize)[!seq(studySize) %in% trainIndex]

print(paste("Training size:", length(trainIndex)))
print(paste("Test size:", length(testIndex)))
```

```{r}
fitCox <- glmnet(as.matrix(TcgaData$EXP[trainIndex,]),
                 Surv(TcgaData$CLINIC$OS_MONTHS[trainIndex],TcgaData$CLINIC$OS_STATUS[trainIndex]),
                 family = "cox", alpha = 1, lambda = 0.15)
predCox <- predict(fitCox,as.matrix(TcgaData$EXP[trainIndex,]), type = "response")
```


**YOUR WORK** Evaluate the efficacy of your predictions with survival data. Use the concordance index is a classical
metric (look at the slides). 1 is a perfect concordance, 0.5 is random.

```{r}
#### YOUR WORK
```

The concordance in the training and test sets are both relatively low.

16) What could be the reasons of poor concordance even on the training set?

**ANSWER**

# End

In this workshop you have learned 2 classical machine learning approaches: classification and regression. We have
addressed the challenge of modeling high dimension data (gene expression) to predict clinically relevant observations.

I hope you have gained experience on the good practices in machine learning and the main residing challenges. Maybe
these scripts can be useful for your current or future research, don't hesitate to use it.

Thank you!

`r if (knitr::is_html_output()) '# References {-}'`
